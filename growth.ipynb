##DDD
from google.colab import drive
drive.mount('/content/gdrive')


import pandas as pd
data = pd.read_excel('/content/gdrive/My Drive/DDDDFolder/DDDD.xlsx')
data.head()


##apriori
df1 = data[['Customer','SalesTransactionID','SalesItem']]
df1.head()


df=df1.groupby ('SalesTransactionID') ['SalesItem']. apply (lambda x: x.reset_index (drop = True)). unstack (). reset_index ()
df.drop('SalesTransactionID',axis=1, inplace=True)
df.head()

items = (df[0].unique())
items


encoded_vals = []
def custom():
    for index, row in df.iterrows():
        labels = {}
        uncommons = list(set(items) - set(row))
        commons = list(set(items).intersection(row))
        for uc in uncommons:
            labels[uc] = 0
        for com in commons:
            labels[com] = 1
        encoded_vals.append(labels)
custom()
ohe_df = pd.DataFrame(encoded_vals)
print(ohe_df)


from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules


freq_items = apriori(ohe_df, min_support=0.006, use_colnames=True) #, verbose=1 #min_support=0.006, 
freq_items


freq_items.to_excel('Export&Supportvalues.xlsx')


rules = association_rules(freq_items, metric="confidence", min_threshold=0.006)
rules


import matplotlib.pyplot as plt
plt.scatter(rules['support'], rules['confidence'], alpha=0.2,color='brown')
plt.xlabel('support')
plt.ylabel('confidence')
plt.title('Support vs Confidence')
plt.show()


plt.scatter(rules['support'], rules['lift'], alpha=0.2,color='orange')
plt.xlabel('support')
plt.ylabel('lift')
plt.title('Support vs Lift')
plt.show()


%%capture
!sudo apt-get update --fix-missing

!apt-get install openjdk-8-jdk-headless -qq > /dev/null


!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz


#market basket analys
!mv spark-3.0.0-bin-hadoop3.2.tgz sparkkk
!tar xf sparkkk
!pip install -q findspark


import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.0-bin-hadoop3.2"

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName('fpgrowth') \
    .getOrCreate()

spark  


from google.colab import files
from pyspark.sql import functions as F
from pyspark.ml.fpm import FPGrowth
import pandas


sparkdata = spark.createDataFrame(data)
basketdata = sparkdata.dropDuplicates(['SalesTransactionID', 'SalesItem']).sort('SalesTransactionID')
basketdata = basketdata.groupBy("SalesTransactionID").agg(F.collect_list("SalesItem")).sort('SalesTransactionID')


fpGrowth = FPGrowth(itemsCol="collect_list(SalesItem)", minSupport=0.006, minConfidence=0.006) 
model = fpGrowth.fit(basketdata)

model.freqItemsets.show()
items = model.freqItemsets
model.associationRules.show()
rules = model.associationRules
model.transform(basketdata).show()
transformed = model.transform(basketdata)


result_pdf = items.select("*").toPandas()
result_pdf.head()


result_pdf.to_excel('result_pdfItemsFreq.xlsx')


rules_pdf = rules.select("*").toPandas()
rules_pdf.head()


rules_pdf.to_excel('rules_pdfAnteConseConfLift.xlsx')


transformed_pdf = transformed.select("*").toPandas()
transformed_pdf.head()


transformed_pdf.to_excel('transformed_pdfSalesTransactionIDCollectListPred.xlsx')